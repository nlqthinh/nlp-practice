{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":858345,"sourceType":"datasetVersion","datasetId":454940},{"sourceId":7808106,"sourceType":"datasetVersion","datasetId":4572888}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"here's the link if you want to test: https://www.kaggle.com/code/nlordqting/phobertv2-vietnamese-sentiment-analysis","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-03-13T06:43:01.857874Z","iopub.execute_input":"2024-03-13T06:43:01.858337Z","iopub.status.idle":"2024-03-13T06:43:01.884822Z","shell.execute_reply.started":"2024-03-13T06:43:01.858282Z","shell.execute_reply":"2024-03-13T06:43:01.883279Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install evaluate\n!pip install -U git+https://github.com/huggingface/transformers.git\n!pip install -U git+https://github.com/huggingface/accelerate.git\n!pip install underthesea\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:43:01.887344Z","iopub.execute_input":"2024-03-13T06:43:01.890215Z","iopub.status.idle":"2024-03-13T06:44:46.718407Z","shell.execute_reply.started":"2024-03-13T06:43:01.890161Z","shell.execute_reply":"2024-03-13T06:44:46.717456Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\nCollecting git+https://github.com/huggingface/transformers.git\n  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-tk79619_\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-tk79619_\n  Resolved https://github.com/huggingface/transformers.git to commit d522afea1324b8156c929f3896df14762c9ea716\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.39.0.dev0) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.39.0.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.39.0.dev0-py3-none-any.whl size=8661673 sha256=008fde85b5c2bf36ad3a5749b4ab370faadbc04329687fffb8f74cf5f798d6af\n  Stored in directory: /tmp/pip-ephem-wheel-cache-61e5919d/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.1\n    Uninstalling transformers-4.38.1:\n      Successfully uninstalled transformers-4.38.1\nSuccessfully installed transformers-4.39.0.dev0\nCollecting git+https://github.com/huggingface/accelerate.git\n  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-fr_axt1h\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-fr_axt1h\n  Resolved https://github.com/huggingface/accelerate.git to commit ee163b66fb7848892519e804688cb4ae981aacbe\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.0.dev0) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.0.dev0) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.0.dev0) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.0.dev0) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.0.dev0) (0.20.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.29.0.dev0) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate==0.29.0.dev0) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.29.0.dev0) (2024.2.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.29.0.dev0) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate==0.29.0.dev0) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.29.0.dev0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.29.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.29.0.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.29.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate==0.29.0.dev0) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.29.0.dev0) (1.3.0)\nBuilding wheels for collected packages: accelerate\n  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.29.0.dev0-py3-none-any.whl size=290151 sha256=279ededfd5ad21107c5f616cbe9a9324e85af58d8f50cbde3579ea901e4461c9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-7efpa3qb/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\nSuccessfully built accelerate\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.27.2\n    Uninstalling accelerate-0.27.2:\n      Successfully uninstalled accelerate-0.27.2\nSuccessfully installed accelerate-0.29.0.dev0\nCollecting underthesea\n  Downloading underthesea-6.8.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.10/site-packages (from underthesea) (8.1.7)\nCollecting python-crfsuite>=0.9.6 (from underthesea)\n  Downloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from underthesea) (3.2.4)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from underthesea) (4.66.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from underthesea) (2.31.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.3.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from underthesea) (1.2.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from underthesea) (6.0.1)\nCollecting underthesea-core==1.0.4 (from underthesea)\n  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->underthesea) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->underthesea) (2024.2.2)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (1.11.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->underthesea) (3.2.0)\nDownloading underthesea-6.8.0-py3-none-any.whl (20.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_crfsuite-0.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: underthesea-core, python-crfsuite, underthesea\nSuccessfully installed python-crfsuite-0.9.10 underthesea-6.8.0 underthesea-core-1.0.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from underthesea import word_tokenize\n\ntext = \"Tôi là sinh viên.\"\nwords = word_tokenize(text)\nprint(words)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:44:46.720090Z","iopub.execute_input":"2024-03-13T06:44:46.721040Z","iopub.status.idle":"2024-03-13T06:44:48.492640Z","shell.execute_reply.started":"2024-03-13T06:44:46.720998Z","shell.execute_reply":"2024-03-13T06:44:48.491576Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['Tôi', 'là', 'sinh viên', '.']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom transformers import EvalPrediction\nfrom evaluate import load\nimport numpy as np\nimport re\n\n# Preprocessing functions\ndef preprocess_text(text):\n    # Convert to lowercase\n    text = text.lower()\n    # Remove punctuations\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Remove HTML tags\n    text = re.sub(r'<.*?>', '', text)\n    # Remove URLs\n    text = re.sub(r\"https://\\S+|www\\.\\S+\", '', text)\n    # Remove emojis\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text\n\n# Custom Dataset for Vietnamese Sentiment Analysis\nclass VietnameseSentimentAnalysisDataset(Dataset):\n    def __init__(self, directory, tokenizer, max_length=256):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.texts = []\n        self.labels = []\n        \n        # Load positive and negative files\n        for label, sentiment in enumerate([\"neg\", \"pos\"]):\n            sentiment_dir = os.path.join(directory, sentiment)\n            for filename in os.listdir(sentiment_dir):\n                file_path = os.path.join(sentiment_dir, filename)\n                with open(file_path, 'r', encoding='utf-8') as file:\n                    text = file.read().strip()\n                    # Preprocess text\n                    text = preprocess_text(text)\n                    self.texts.append(text)\n                    self.labels.append(label)\n                    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        \n        # Use underthesea for word segmentation\n        segmented_text = word_tokenize(text)\n        # Tokenize and encode the text\n        tokenized_text = self.tokenizer(\" \".join(segmented_text), max_length=self.max_length, padding='max_length', truncation=True)\n        \n        # Convert to torch tensor\n        item = {key: torch.tensor(val) for key, val in tokenized_text.items()}\n        item['labels'] = torch.tensor(label)\n        return item\n\n# Initialize tokenizer with BARTpho\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base-v2\")\n\n# Example dataset initialization\ntrain_dataset = VietnameseSentimentAnalysisDataset(\n    directory=\"/kaggle/input/vietnamese-sentiment-analysis-lab-thay-huong/data_train/data_train/train\", \n    tokenizer=tokenizer\n)\n\nval_dataset = VietnameseSentimentAnalysisDataset(\n    directory=\"/kaggle/input/vietnamese-sentiment-analysis-lab-thay-huong/data_train/data_train/test\", \n    tokenizer=tokenizer\n)\n\ntest_dataset = VietnameseSentimentAnalysisDataset(\n    directory=\"/kaggle/input/vietnamese-sentiment-analysis-lab-thay-huong/data_test/data_test/test\", \n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:44:48.495417Z","iopub.execute_input":"2024-03-13T06:44:48.495962Z","iopub.status.idle":"2024-03-13T06:49:18.654128Z","shell.execute_reply.started":"2024-03-13T06:44:48.495931Z","shell.execute_reply":"2024-03-13T06:49:18.653109Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-03-13 06:44:58.047956: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-13 06:44:58.048065: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-13 06:44:58.206975: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/678 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d10504fd816a4d59a8fa20babdf33142"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/895k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c1b41f0d2af451e8cba944e97bb8038"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"bpe.codes:   0%|          | 0.00/1.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e26945c8b3a4752a56e48a24fc11353"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.13M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d71b51d0f57a4d71a1656a16353779a4"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Custom trainer \n","metadata":{}},{"cell_type":"markdown","source":"Insprired by https://www.kaggle.com/code/minhcng/sentiment-analysis-using-roberta-lstm","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    sample = train_dataset[i]\n    print(f\"Sample {i+1}:\")\n    print(f\"Text: {tokenizer.decode(sample['input_ids'], skip_special_tokens=True)}\")\n    print(f\"Label: {sample['labels'].item()}\")\n    print(\"\")","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:49:18.655358Z","iopub.execute_input":"2024-03-13T06:49:18.656130Z","iopub.status.idle":"2024-03-13T06:49:18.772324Z","shell.execute_reply.started":"2024-03-13T06:49:18.656099Z","shell.execute_reply":"2024-03-13T06:49:18.771431Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Sample 1:\nText: this place is quite nice flan gato is acceptable and almost the others are unauthentic ones i suppuse i called a tiramisu with chocolate flavor and it was the worst tiramisu i ve ever eaten anyhow i recommend this store for everyone who needs a place for hanging out with some sweetie cake\nLabel: 0\n\nSample 2:\nText: đă ăn và thấy cũng tạm tạm nhưng đặc_biệt là không thích thái độ phục_vụ của nhân_viên trong quán kêu 2 3 lần tô cháo mà nhìn rồi quay mặt đi giống như mình đang đi xin\nLabel: 0\n\nSample 3:\nText: lần đầu được chở đi ăn_ở đây là lúc sinh_nhật con bạn chẳng hiểu là do ngon hay vui mà lúc đó thấy món nào cũng ngon chỉ có mì jajang hơi mặn chút trưa này có dịp đi ngang ghé vào ăn lần hai thì hơi gượng lúc bước vào nghe bảo mình đi một_mình thì bạm nb có_vẻ khựng lại vào kêu 1 tokk ramen vs 1 cơm bokkum cơm thì lần đầu ăn ngon lắm cơm nóng_hổi có cả lớp cơm cháy phía dưới còn hôm_nay thì nguội_ngắt còn hơi nhạt nữa về phần tokk thì ko còn gì để nói ngoài thấy ghê có_vẻ quán bỏ bột quá_tay vào phần sốt nên nó vừa nhạt vừa bết còn tokk thì luộc chưa chín phải kể thêm là siêu ít nhá nhân_viên thì do bận quá nên chẳng nhớ khách gọi thêm cái gì lúng_ta_lúng_túng mình nghĩ quán nên train thêm cho các bạn ấy tính cả bữa là 75 k thằng bạn bảo rẻ nhưng nói thiệt cũng vậy à thêm chút_xíu vào tokkpoki hay hanuri ăn còn có_lý hơn\nLabel: 0\n\nSample 4:\nText: thất_vọng vì đang chơi game mà hay bị rút wifi mình nói thì cái mặt nhân_viên đó xưng lên xem_thường và muốn ăn_tươi_nuốt_sống mình sợ quá\nLabel: 0\n\nSample 5:\nText: quán này không ấn_tượng lắm với mình ấn_tượng đầu_tiên là menu quá chán menu nhìn rất cũ và có cảm_giác hơi dơ quán nên xem_lại chuyện này vì cái nhìn đầu_tiên của khách là menu hôm mình ăn mình có gọi bún đậu tá lả và phở cuốn có_thể do mình thích ăn đồ_ăn nóng_hổi nên sẽ mất thiện_cảm với quán thịt giò lạnh_ngắt như lấy từ tủ_lạnh đem ra cho khách còn đậu cũng chỉ ấm nhẹ mình có cảm_tưởng đang ăn 1 phần ăn đã để ra ngoài 1 lúc chứ không phải như mới được nấu đem ra và mình phải nhờ bạn nhân_viên hâm_nóng lại hộ mình điểm cộng là 1 phần ăn khá nhiều còn về phở cuốn có vị khá lạ_miệng và cũng_như mình đã nói nó cũng lạnh_tanh thật_ra món này mình không yêu_cầu nóng_hổi nhưng nó cũng lạnh như đồ bỏ tủ_lạnh vậy cả buổi hôm đó mình ăn nhưng cảm_thấy rất khó_chịu bụng nên chắc sẽ không quay lại thêm lần nào nữa nếu quán không khắc_phục được\nLabel: 0\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# from evaluate import load\n# import numpy as np\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    # Load metrics\n    precision_metric = load(\"precision\")\n    recall_metric = load(\"recall\")\n    f1_metric = load(\"f1\")\n    accuracy_metric = load(\"accuracy\")\n    \n    # Compute metrics\n    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"precision\"]\n    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"recall\"]\n    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")[\"f1\"]\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    \n    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy}\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:49:18.773535Z","iopub.execute_input":"2024-03-13T06:49:18.773871Z","iopub.status.idle":"2024-03-13T06:49:18.781853Z","shell.execute_reply.started":"2024-03-13T06:49:18.773845Z","shell.execute_reply":"2024-03-13T06:49:18.780808Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nclass SentimentTrainer:\n    def __init__(self, config):\n        self.config = config\n        self.model = AutoModelForSequenceClassification.from_pretrained(config.pretrained_path, num_labels=config.model_num_labels)\n        self.tokenizer = AutoTokenizer.from_pretrained(config.pretrained_path)\n        \n        # Load datasets\n        self.train_dataset, self.eval_dataset = self.load_dataset()\n        \n        # Define TrainingArguments\n        training_args = TrainingArguments(\n            output_dir=config.training_output_dir,\n            evaluation_strategy=\"steps\",\n            eval_steps=config.training_eval_steps,  # Evaluate every eval_steps\n            per_device_train_batch_size=config.training_batch_size,\n            per_device_eval_batch_size=config.training_batch_size,\n            num_train_epochs=config.training_num_epochs,\n            save_steps=config.training_save_steps,\n            logging_dir=config.training_logging_dir,\n            logging_steps=config.training_logging_steps,\n            load_best_model_at_end=True,\n            metric_for_best_model=\"f1\",\n            report_to=\"none\",  # Disable wandb\n        )\n        \n        # Initialize the Trainer\n        self.trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset= val_dataset,\n            tokenizer=self.tokenizer,\n            compute_metrics=compute_metrics  # Use the compute_metrics function\n        )\n    \n    def load_dataset(self):\n#         self.train_dataset = create_sentiment_dataset(self.config.train_data_path, self.tokenizer, \"train\")\n#         self.eval_dataset = create_sentiment_dataset(self.config.eval_data_path, self.tokenizer, \"eval\")\n        self.train_dataset =  train_dataset\n        self.eval_dataset =  val_dataset\n        return self.train_dataset, self.eval_dataset\n\n    \n    def train(self):\n        self.trainer.train()\n\n    def eval(self):\n        evaluation_results = self.trainer.evaluate(eval_dataset=self.eval_dataset)\n        print(evaluation_results)\n        return evaluation_results\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:49:18.783204Z","iopub.execute_input":"2024-03-13T06:49:18.783515Z","iopub.status.idle":"2024-03-13T06:49:18.795198Z","shell.execute_reply.started":"2024-03-13T06:49:18.783488Z","shell.execute_reply":"2024-03-13T06:49:18.794370Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class SentimentConfig:\n    def __init__(self):\n        self.pretrained_path = \"vinai/phobert-base-v2\"\n        self.model_num_labels = 2  # Assuming binary classification\n        self.training_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.training_output_dir = \"/kaggle/working/results\"\n        self.training_eval_steps = 50  # Evaluate every 50 steps\n        self.training_batch_size = 16\n        self.training_num_epochs = 1\n        self.training_save_steps = 1000\n        self.training_logging_dir = \"/kaggle/working/logs\"\n        self.training_logging_steps = 100\n\n# Create an instance of this configuration\nconfig = SentimentConfig()","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:49:18.796504Z","iopub.execute_input":"2024-03-13T06:49:18.796874Z","iopub.status.idle":"2024-03-13T06:49:18.857137Z","shell.execute_reply.started":"2024-03-13T06:49:18.796848Z","shell.execute_reply":"2024-03-13T06:49:18.856204Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Train the model","metadata":{}},{"cell_type":"code","source":"\ntrainer = SentimentTrainer(config)\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T06:49:18.858462Z","iopub.execute_input":"2024-03-13T06:49:18.858859Z","iopub.status.idle":"2024-03-13T07:49:14.784492Z","shell.execute_reply.started":"2024-03-13T06:49:18.858832Z","shell.execute_reply":"2024-03-13T07:49:14.783452Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/540M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42311c2988334bff8d6abfbcd97688a6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='938' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [938/938 59:47, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>No log</td>\n      <td>0.317216</td>\n      <td>0.884579</td>\n      <td>0.871700</td>\n      <td>0.870617</td>\n      <td>0.871700</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.383200</td>\n      <td>0.332640</td>\n      <td>0.888670</td>\n      <td>0.873800</td>\n      <td>0.872581</td>\n      <td>0.873800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.383200</td>\n      <td>0.273030</td>\n      <td>0.897133</td>\n      <td>0.896100</td>\n      <td>0.896032</td>\n      <td>0.896100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.309900</td>\n      <td>0.291478</td>\n      <td>0.897332</td>\n      <td>0.888200</td>\n      <td>0.887554</td>\n      <td>0.888200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.309900</td>\n      <td>0.269622</td>\n      <td>0.898246</td>\n      <td>0.898200</td>\n      <td>0.898197</td>\n      <td>0.898200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.278800</td>\n      <td>0.242169</td>\n      <td>0.909118</td>\n      <td>0.905300</td>\n      <td>0.905079</td>\n      <td>0.905300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.278800</td>\n      <td>0.254306</td>\n      <td>0.907262</td>\n      <td>0.906900</td>\n      <td>0.906879</td>\n      <td>0.906900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.253800</td>\n      <td>0.252348</td>\n      <td>0.911169</td>\n      <td>0.910100</td>\n      <td>0.910042</td>\n      <td>0.910100</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.253800</td>\n      <td>0.260174</td>\n      <td>0.897082</td>\n      <td>0.894200</td>\n      <td>0.894008</td>\n      <td>0.894200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.252300</td>\n      <td>0.258773</td>\n      <td>0.905170</td>\n      <td>0.904400</td>\n      <td>0.904355</td>\n      <td>0.904400</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.252300</td>\n      <td>0.259706</td>\n      <td>0.908843</td>\n      <td>0.908600</td>\n      <td>0.908586</td>\n      <td>0.908600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.238600</td>\n      <td>0.232049</td>\n      <td>0.913807</td>\n      <td>0.912500</td>\n      <td>0.912431</td>\n      <td>0.912500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.238600</td>\n      <td>0.248486</td>\n      <td>0.907451</td>\n      <td>0.907200</td>\n      <td>0.907186</td>\n      <td>0.907200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.229800</td>\n      <td>0.229566</td>\n      <td>0.913241</td>\n      <td>0.913200</td>\n      <td>0.913198</td>\n      <td>0.913200</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.229800</td>\n      <td>0.235613</td>\n      <td>0.914604</td>\n      <td>0.914600</td>\n      <td>0.914600</td>\n      <td>0.914600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.220500</td>\n      <td>0.230179</td>\n      <td>0.913936</td>\n      <td>0.911300</td>\n      <td>0.911159</td>\n      <td>0.911300</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.220500</td>\n      <td>0.230616</td>\n      <td>0.915943</td>\n      <td>0.915900</td>\n      <td>0.915898</td>\n      <td>0.915900</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.213400</td>\n      <td>0.224766</td>\n      <td>0.916229</td>\n      <td>0.916200</td>\n      <td>0.916199</td>\n      <td>0.916200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e96ee700a90475abb724c447a8298af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30057d0dc00245dd9a949f0a01bb6cc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd69cfb93b9c496294f33b899f02b54e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea46454a6cf14bc18d8f4a6fe296d58b"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Test the model","metadata":{}},{"cell_type":"code","source":"#Make predictions\npredictions, label_ids, metrics = trainer.trainer.predict(test_dataset=test_dataset)\n\n# Optionally, compute metrics based on predictions and labels\n# This assumes you have a way to calculate metrics from predictions, similar to compute_metrics\ntest_metrics = compute_metrics((predictions, label_ids))\n\nprint(\"Test Metrics:\", test_metrics)\n\n# def manual_predict(model, dataloader, device):\n#     model.to(device)\n#     model.eval()\n#     predictions = []\n#     with torch.no_grad():\n#         for batch in dataloader:\n#             inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n#             outputs = model(**inputs)\n#             logits = outputs.logits\n#             predictions.append(logits.argmax(dim=-1).cpu().numpy())\n#     return np.concatenate(predictions)\n\n# # Adjust the batch size for the DataLoader\n# test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)  # Smaller batch size\n\n# # Assuming your model is stored in `trainer.model`\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# predictions = manual_predict(trainer.model, test_dataloader, device)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T07:49:14.787337Z","iopub.execute_input":"2024-03-13T07:49:14.787630Z","iopub.status.idle":"2024-03-13T07:51:47.614740Z","shell.execute_reply.started":"2024-03-13T07:49:14.787603Z","shell.execute_reply":"2024-03-13T07:51:47.613764Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Test Metrics: {'precision': 0.9220492278219332, 'recall': 0.922, 'f1': 0.9219977254536741, 'accuracy': 0.922}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Save the model","metadata":{}},{"cell_type":"code","source":"# Save the model and tokenizer\nmodel_save_path = \"/kaggle/working/sentiment_model\"\ntrainer.model.save_pretrained(model_save_path)\ntrainer.tokenizer.save_pretrained(model_save_path)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T07:51:47.615842Z","iopub.execute_input":"2024-03-13T07:51:47.616149Z","iopub.status.idle":"2024-03-13T07:51:48.735944Z","shell.execute_reply.started":"2024-03-13T07:51:47.616123Z","shell.execute_reply":"2024-03-13T07:51:48.735066Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/sentiment_model/tokenizer_config.json',\n '/kaggle/working/sentiment_model/special_tokens_map.json',\n '/kaggle/working/sentiment_model/vocab.txt',\n '/kaggle/working/sentiment_model/bpe.codes',\n '/kaggle/working/sentiment_model/added_tokens.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Load the model for inference if you want","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Load the trained model and tokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(model_save_path)\ntokenizer = AutoTokenizer.from_pretrained(model_save_path)\n\n# Ensure the model is in evaluation mode\nmodel.eval()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T07:51:48.737132Z","iopub.execute_input":"2024-03-13T07:51:48.737439Z","iopub.status.idle":"2024-03-13T07:51:49.237716Z","shell.execute_reply.started":"2024-03-13T07:51:48.737410Z","shell.execute_reply":"2024-03-13T07:51:49.236678Z"},"trusted":true},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n      (position_embeddings): Embedding(258, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"# Performing Inference on Test Samples\n","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef predict(sentences):\n    inputs = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n        return predictions\n\n# Example sentences from test set: sentence 1 and 2 has \"neg\" label and sentence 3 has \"pos\" label\ntest_sentences = [\"\"\"\nQuán này khá là nổi_tiếng nay mới có dịp ghé thử . Năm ở vii trí khá dễ tìm , quán hơi nhỏ , đi chiều_tối là đông lắm phải đợi lâu\nNhân_viên thì bình_thường à vì do đông quá nên cũng thông_cảm\nMón ăn :\nMình gọi gà sốt phô_mai cay : gà quá bở , thịt ăn vào k thơm , món này đã có mỡ rồi cộng thêm da_gà nữa ăn rất ngáy . Cảm_giác ngán lắm\nCơm trộn kim_chi HQ thì_phải : ăn hơi cay , nóng_hổi luôn , vị ăn ok nhưng k đặc_sắc . Với giá đó thì ăn k đáng tiền .\nKhoai_tây chiên : dở và ăn vị k ngon\nTóm_lại k như kì_vọng của mình , giá_cả ở đây quá đắt so với chất_lượng . Mà đi ăn phải chầu_chực như_thế thì cũng hơi mệt\nÀ có nước_lọc miễn_phí nha . Mọi người tới ăn nhớ hỏi .\"\"\"\n                  ,\"\"\"Chưa có nhìu món về HQ , vẫn mang đậm_nét ẩm_thực của quán Việt ( tên thì rất Korea ) , thực_đơn sơ_sài ... hi_vọng sẽ cập_nhật nhìu món ăn kiểu \" vỉa_hè \" mang đậm chất Korea\"\"\"\n                  , \"An ngon , gia re , sach se mat me con gi bang\"\n            \n                 ]\n\n# Get predictions\npredictions = predict(test_sentences)\n\n# Process predictions\nfor sentence, prediction in zip(test_sentences, predictions):\n    print(f\"Sentence: {sentence}\")\n    print(f\"Prediction: {prediction.numpy()}\")\n    print(\"This is a positive review\" if prediction.argmax() == 1 else \"This is a negative review\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-13T07:51:49.239122Z","iopub.execute_input":"2024-03-13T07:51:49.239401Z","iopub.status.idle":"2024-03-13T07:51:50.074278Z","shell.execute_reply.started":"2024-03-13T07:51:49.239376Z","shell.execute_reply":"2024-03-13T07:51:50.072368Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Sentence: \nQuán này khá là nổi_tiếng nay mới có dịp ghé thử . Năm ở vii trí khá dễ tìm , quán hơi nhỏ , đi chiều_tối là đông lắm phải đợi lâu\nNhân_viên thì bình_thường à vì do đông quá nên cũng thông_cảm\nMón ăn :\nMình gọi gà sốt phô_mai cay : gà quá bở , thịt ăn vào k thơm , món này đã có mỡ rồi cộng thêm da_gà nữa ăn rất ngáy . Cảm_giác ngán lắm\nCơm trộn kim_chi HQ thì_phải : ăn hơi cay , nóng_hổi luôn , vị ăn ok nhưng k đặc_sắc . Với giá đó thì ăn k đáng tiền .\nKhoai_tây chiên : dở và ăn vị k ngon\nTóm_lại k như kì_vọng của mình , giá_cả ở đây quá đắt so với chất_lượng . Mà đi ăn phải chầu_chực như_thế thì cũng hơi mệt\nÀ có nước_lọc miễn_phí nha . Mọi người tới ăn nhớ hỏi .\nPrediction: [0.9952714 0.0047286]\nThis is a negative review\nSentence: Chưa có nhìu món về HQ , vẫn mang đậm_nét ẩm_thực của quán Việt ( tên thì rất Korea ) , thực_đơn sơ_sài ... hi_vọng sẽ cập_nhật nhìu món ăn kiểu \" vỉa_hè \" mang đậm chất Korea\nPrediction: [0.97268885 0.02731115]\nThis is a negative review\nSentence: An ngon , gia re , sach se mat me con gi bang\nPrediction: [0.15724444 0.8427556 ]\nThis is a positive review\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"Not bad!!!","metadata":{}}]}